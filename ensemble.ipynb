{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import toolbox_02450 as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensemble:\n",
    "    def plot_confusion_matrix(\n",
    "        self, cm, title=\"Confusion matrix\", cmap=plt.cm.get_cmap(name=\"Blues\")\n",
    "    ):\n",
    "        plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar\n",
    "        tick_marks = np.arange(2)\n",
    "        plt.xticks(tick_marks, [\"positive\", \"negative\"])  # , rotation=45)\n",
    "        plt.yticks(tick_marks, [\"positive\", \"negative\"])\n",
    "        plt.tight_layout\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        fmt = \"d\"  #'.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.0\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                format(cm[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "            )\n",
    "        plt.show()\n",
    "\n",
    "    def conf_matrix_stats(self, TP, FN, FP, TN, stats):\n",
    "        \"\"\"\n",
    "        makes confusion matrix\n",
    "\n",
    "        TP = true positive\n",
    "        FN = false negative\n",
    "        FP = false positive\n",
    "        FN = false negative\n",
    "        stats = list of stats to output, should be inputtet as string. Options are:\n",
    "                precision = \"p\"\n",
    "                recall = \"r\"\n",
    "                accuracy = \"acc\"\n",
    "                error = \"err\"\n",
    "                true positive rate = \"tpr\"\n",
    "                false positive rate = \"fpr\"\n",
    "                show confususion matrix = \"show\"\n",
    "                F-measure = \"f_meas\"\n",
    "                Receiver operating characteristic plot (TPR~FPR plot) = \"roc\"\n",
    "                show table with all values (list not necessary)= \"all\"\n",
    "        \"\"\"\n",
    "\n",
    "        # calculations\n",
    "        cm = np.array([[TP, FN], [FP, TN]])\n",
    "        N = TP + FN + FP + TN\n",
    "        p = TP / (TP + FP)\n",
    "        r = TP / (TP + FN)\n",
    "        acc = (TP + TN) / N\n",
    "        err = (FN + FP) / N\n",
    "        TPR = TP / (TP + FN)\n",
    "        FPR = FP / (TN + FP)\n",
    "        F = (2*p*r)/(p+r)\n",
    "\n",
    "        if \"p\" in stats:\n",
    "            print(\"The precision is {}\".format(p))\n",
    "\n",
    "        if \"r\" in stats:\n",
    "            print(\"The recall is {}\".format(r))\n",
    "\n",
    "        if \"acc\" in stats:\n",
    "            print(\"The accuracy is {}\".format(acc))\n",
    "\n",
    "        if \"err\" in stats:\n",
    "            print(\"The error is {}\".format(err))\n",
    "\n",
    "        if \"tpr\" in stats:\n",
    "            print(\"The true positive rate is {}\".format(TPR))\n",
    "\n",
    "        if \"fpr\" in stats:\n",
    "            print(\"The false positive rate is {}\".format(FPR))\n",
    "\n",
    "        if \"show\" in stats:\n",
    "            self.plot_confusion_matrix(cm)\n",
    "\n",
    "        if \"roc\" in stats:\n",
    "            plt.plot(FPR, TPR)\n",
    "            plt.title(\"Receiver operating characteristic\")\n",
    "            plt.ylabel(\"TPR\")\n",
    "            plt.xlabel(\"FPR\")\n",
    "            plt.show()\n",
    "        if \"f_meas\" in stats:\n",
    "            print (\"The F measure is {}\".format(F))\n",
    "        if \"all\" in stats:\n",
    "            all = pd.DataFrame(\n",
    "                {\n",
    "                    \"Stat\": [\"Precision\", \"Recall\", \"Accuracy\", \"Error\", \"TPR\", \"FPR\",\"F-measure\"],\n",
    "                    \"Value\": [p, r, acc, err, TPR, FPR,F],\n",
    "                }\n",
    "            )\n",
    "            print(all)\n",
    "\n",
    "    def make_conf_matrix(self, true_val, pred_val, show_conf_matrix=False):\n",
    "        \"\"\"\n",
    "        calculates values for a confusion matrix and various stats\n",
    "        ------------------------------------------------\n",
    "        parameters:\n",
    "        --------------------------------------------\n",
    "        true_val = list of the correct labels, must be binarised so that 1 = positve class and 0 = negative class\n",
    "        pred_val = list of the predicted labels, must be binarised so that 1 = positve class and 0 = negative class\n",
    "\n",
    "        returns the amount of true positives, false positives, false negatives and true negatives\n",
    "        \"\"\"\n",
    "        true_val = np.array(true_val)\n",
    "        pred_val = np.array(pred_val)\n",
    "\n",
    "        pred_pos = true_val[pred_val == 1]\n",
    "        pred_neg = true_val[pred_val == 0]\n",
    "\n",
    "        TP = np.sum(pred_pos == 1)\n",
    "        FP = np.sum(pred_pos == 0)\n",
    "        FN = np.sum(pred_neg == 1)\n",
    "        TN = np.sum(pred_neg == 0)\n",
    "\n",
    "        if show_conf_matrix:\n",
    "            stats = [\"all\", \"show\"]\n",
    "        else:\n",
    "            stats = \"all\"\n",
    "\n",
    "        self.conf_matrix_stats(TP=TP, FP=FP, FN=FN, TN=TN, stats=stats)\n",
    "\n",
    "        return TP, FP, FN, TN\n",
    "\n",
    "    def plot_roc(self, true_val, pred_val):\n",
    "        \"\"\"\n",
    "        calculates the fpr and tpr and plots a roc curve\n",
    "        to compare the outputtet graph with the possible answers, look at where the plot has a elbow\n",
    "        -----------------------------------------------\n",
    "        parameters:\n",
    "        -----------\n",
    "        true_val = list of the correct labels, must be binarised so that 1 = positve class and 0 = negative class\n",
    "        pred_val = list of the predicted labels, must be binarised so that 1 = positve class and 0 = negative class\n",
    "\n",
    "        returns the area under the curve (AUC)\n",
    "        \"\"\"\n",
    "        fpr, tpr, _ = metrics.roc_curve(true_val, pred_val)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        plt.title(\"Receiver Operating Characteristic\")\n",
    "        plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.plot([0, 1], [0, 1], \"r--\")\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.show()\n",
    "\n",
    "        return roc_auc\n",
    "    def plot_roc_pred(self,truth,probabilities):\n",
    "        \"\"\"Generates a ROC curve from true labels and predicted class probabilities\n",
    "\n",
    "        Args:\n",
    "            truth (list): List with true class labels (can also be a prediction from a model)\n",
    "            probabilities (list): List with predicted class probabilities\n",
    "        \"\"\"\n",
    "        plt.figure(1)\n",
    "        tb.rocplot(probabilities, truth)\n",
    "\n",
    "        plt.show()  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
